# Getting Started

## Insertion Sort
**Sorting Problem**
**Input:** A sequence of n numbers (a1, a2, ..... an)
**Output:** A permutation (reordering) (a1, a2, .... an) of the input sequence such as a, <= a2 .... <= an.

**Insertion Sort**
```
for j = 2 to A.length
    key = A[j]
    //Insert A[j] into the sorted sequence A[1..j-1]
    i = j - 1
    while i > 0 and A[i] > key
        A[i + 1] = A[i]
        i = i - 1
    A[i + 1] = key
```

Numbers that we want to sort are *keys*.

Difference between pseudo-code vs "real" code is that we ignore syntax and use whatever is clear/concise to specify a given algorithm. Not actually concerned with software engineering.        

*In place* means that making changes within the provided structure rather than making a new structure. 

**Three outcomes that must be proven about a loop invariant:**
- *Initialization:* it is true prior to the first iteration of the loop
- *Maintenance:* It is true before an iteration of the loop, it remains true before the next iteration
- *Termination:* When the loop terminates, the invariant gives us a useful property that helps show that the algorithm is correct.

### Exercises
1.1: On paper

1.2:
```
for j = 2 to A.length
    key = A[j]
    i = j - 1
    while i > 0 and A[i] < key
        A[i + 1] = A[i]
        i = i - 1
    A[i + 1] = key
```

1.3:  
```
linear search(array, target)
    i = 0
    while i < array.length
        return i if array[i] == target
    end
    return NIL

```

1.4:
```
Input: 2 arrays
Output: 1 array containing the sum of the 2 integers
add(array A, array B)
    result = []
    i = 0
    while i < array A. length 
        result[i] = array A[i] + array B [i]
    end
    return result
end
```

## Analyzing Algorithms

**Analyzing** an algorithm means predicting the resources that an algorithm requires. 

In the **random-access machine (RAM)** model of computation instructions are executed one after an-other, with no concurrent operations. 
**RAM model**
- arithmetic 
- data movement (load, store, copy)
- control (conditional and unconditional branch, subroutine call and return)
- Data types (integer and floating point)

The best notion for *input size* depends on problem studied. Input can be the total number of bits needed to represent the input in ordinary binary notation or number of vertices/edges in a graph.

The *running time* on a particular input is the number of steps executed.


**Analysis of Insertion Sort:**
Best case scenario: Linear function 0(n)
Worst case scenario: Quadratic Function O(n^2)

**Worst-case and average-case analysis:**
The *worst case running time* is the most focused metric because:
- Gives the upper-bound of running time. 
- For some algos, worst case occurs fairly often. 
- "Average case" is often roughly as bad as the worst case. 

**Order of Growth:**

**Exercises**
2.1:
Θ(n3)
2.2:
```
Pseudocode:
selection sort(array A)
    i = 1
    for i ... A.length -1
        min = i
        j = i + 1
        for j ... A.length
            if array A[j] < A[min]
                min = j
        temp = A[i]
        A[i] = A[min]
        A[min] = temp
end
```
Loop invariants: A sorted array

Why n-1? Because it is comparing 2 elements you don't need to transfer both pointers to the end just a.length and a.length - 1

Best-case and worst-case are O(n2) <-- nested loops

2.3: On average 50% needs to be checked and 100% needs to be checked for the worst case. --> O(n).

2.4: Make a check at the first step to see if it is already sorted, if so return. 

## Designing Algorithms 
**Divide-and-conquer**
These type of algorithms break the problem into several sub-problems recursively then combine these solutions to create a solution to the original problem. 

**Three Steps to D-A-C at each level of recursion:**
- *Divide* the problem into a number of subproblems that are smaller instances of same problem
- *Conquer* the subproblems by solving them recursively. 
- *Combine* the solution to the subproblems into the solution for the original problem.

**Three Steps to Merge Sort**:
- *Divide* the n-element sequence to be sorted into 2 subsequences of n/2 elements each
- *Conquer* sort the 2 subsequences recursively using merge sort
- *Combine* merge the two sorted subsequences to produce the sorted answer 


Pro: Running times are often easily determined

**Analyzing divide-and-conquer algorithms**
A recursive algorithm can be describes as having a running time by a *recurrence equation* or *recurrence* --> describes the overall run time in terms of run time of smaller inputs. 
Constant run time
O(n)
**Analysis of merge sort**
O(nlogN)

**Exercises**
3.1: On paper

3.2:
```
MERGE(Array, p, q, r)
  n1 = q - p + 1
  n2 = r - q
  let L[1..n₁] and R[1..n₂] be new arrays
  for i = 1 to n₁
      L[i] = Array[p + i - 1]
  for j = 1 to n₂
      R[j] = Array[q + j]
  i = 1
  j = 1
  for k = p to r
      if i > n₁
          Array[k] = R[j]
          j = j + 1
      else if j > n₂
          Array[k] = L[i]
          i = i + 1
      else if L[i] ≤ R[j]
          Array[k] = L[i]
          i = i + 1
      else
          Array[k] = R[j]
          j = j + 1
```

3.3:
On paper 

3.4:
T(n) = {O(1) if n = 1 else
       { T(n - 1) + C(n-1) 
where C(n) is time to insert an element in a sorted array of n elements
3.5:
Psuedocode
```
Binary Search (Array, target)
    low = 1
    high = Array length
    while low <= high
        mid = (low + high) / 2
        if Array[mid] == target
            return mid 
        else if Array[mid] < target
            low = mid + 1
        else
            high = mid - 1
        end
    end
    return 0
```
T(n + 1) = T(n/2) + c
O(NlogN)

3.6:
No, an insertion sort cannot have improved worst-case run time of O(NlogN) because by it's nature it require the movement of elements that are bigger to the right. 

3.7:
Sort the Array
```
2-SUM(S, x)
    A = MERGE-SORT(S)
    for i = 1 to S.length
      if BINARY-SEARCH(A, x - S[i]) != NIL
          return true

  return false
```
O(n) can be achieve by implementing a Hash Table that passes through S once, adding in new integers and checking HASH[target - current] exists 

### Problems

2.1:
a. Worst case run time for insertion sort is an^2 + bn + c so
n/k(ak^2 + bk + c) = ank + bn + cn/k = O(nk)
b. On paper
c. largest value k = log n                                             
d. In practice, k = larger list length which insertion sort is faster than merge sort 

2.2:
a. We need to prove that the same elements in A are in A' but sorted in an ascending manner
b. 
Variant:A[j...n] w/ A[j] is the smallest element
Initialization: j = A.length so A[j] is only 1 element and claim holds
Maintenance:  On each step, we replace A[j] with A[j−1] if smaller.1 is true since we are only swapping 2 values. 2 is true since A[j−1] becomes the smallest of A[j] and A[j−1] and the loop invariant states that A[j] is the smallest one in A[j..end].
Termination: At termination j = i so we know that A[i] is the smallest element of the original array.

c.
Initialization: The subarray A[1..i−1] is empty and element is smallest element of subarray

Maintenance:A[1..i] consists of elements that are smaller than elements of A[i + 1..n] in a sorted order

Termination: Terminates when i = A.length meaning that A[1..n] will be all elements from A in a sorted order

d. Worst case is the same as insertion sort = O(n^2) however the best case of an insertion sort can be O(n) whereas bubblesort cannot do better than O(n^2)

2.3:
a.O(n)
b.
```
NAIVE
    y = 0
    for k = 0 up to n
        temp = 1
        for i = 1 up to k
            temp = temp * x
        y = y + a[k] * temp
```
run time 
c. On paper
Initialization: i = n
Maintenance:
Termination: terminates at i = -1
d. The invariant is a sum equaling a polynomial with the given coefficients. 

2.4:
a. (1,5)(2,5)(3,4) (3,5)(4,5)
b. An array(n...2,1) would have the most inversions. It would have n * (n-1) / 2
c. The inversion number of A is equal to the number of steps of insertion sort
d. 
just modified 2.3.2
```
MERGE-SORT(A, p, r):
  if p < r
      inversions = 0
      q = (p + r) / 2
      inversions += merge_sort(A, p, q)
      inversions += merge_sort(A, q + 1, r)
      inversions += merge(A, p, q, r)
      return inversions
  else
      return 0

MERGE(A, p, q, r)
  n1 = q - p + 1
  n2 = r - q
  let L[1..n₁] and R[1..n₂] be new arrays
  for i = 1 to n₁
      L[i] = A[p + i - 1]
  for j = 1 to n₂
      R[j] = A[q + j]
  i = 1
  j = 1
  for k = p to r
      if i > n₁
          A[k] = R[j]
          j = j + 1
      else if j > n₂
          A[k] = L[i]
          i = i + 1
      else if L[i] ≤ R[j]
          A[k] = L[i]
          i = i + 1
      else
          A[k] = R[j]
          j = j + 1
          inversions += n₁ - i
  return inversions
```